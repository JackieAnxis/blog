<!DOCTYPE html>
<html lang="en">
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="Jiacheng Pan&#39;s Blog">
  <meta name="keyword" content>
  <!-- <link rel="shortcut icon" href="/blog/img/favicon.ico"> -->
  <link rel="shortcut icon" href="https://avatars2.githubusercontent.com/u/19246028?s=460&amp;u=8d4f9462069e17c42c9f7e460369ac703146ceb9&amp;v=4">

  <title>
     杂谈：浙江大学-数据挖掘课程-复习笔记 - Jiacheng Pan&#39;s Blog 
  </title>

  <!-- Custom CSS -->
  <link rel="stylesheet" href="/blog/css/aircloud.css"> <link rel="stylesheet" href="/blog/css/gitment.css">
  <!--<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">-->
  <link href="//at.alicdn.com/t/font_620856_pl6z7sid89qkt9.css" rel="stylesheet" type="text/css">
  <!-- ga & ba script hoook -->
  <script></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body>

<!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="site-nav-toggle" id="site-nav-toggle">
    <button>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
    </button>
</div>

<div class="index-about">
    <i> 🎶 生命因你而火热 💯 </i>
</div>

<div class="index-container">
    
    <div class="index-left">
        <div class="nav" id="nav">
    <div class="avatar-name">
        <div class="avatar ">
            <a href="http://panjiacheng.site"><img
                    src="https://avatars2.githubusercontent.com/u/19246028?s=460&amp;u=8d4f9462069e17c42c9f7e460369ac703146ceb9&amp;v=4" /></a>
        </div>
        <div class="name">
            <a href="http://panjiacheng.site">
                <i>Jiacheng Pan</i>
            </a>
        </div>
    </div>
    <div class="contents" id="nav-content">
        <ul>
            <!-- <li >
                <a
                    href="http://panjiacheng.site">
                    <i class="iconfont icon-shouye2"></i>
                    <span>🏡&nbsp;&nbsp;主页</span>
                </a>
            </li> -->
            <li >
                <a href="/blog/">
                    <!-- <i class="iconfont icon-guidang1"></i> -->
                    <span>⌨️&nbsp;&nbsp;博客</span>
                </a>
            </li>
            <li >
                <a href="/blog/author">
                    <!-- <i class="iconfont icon-guanyu2"></i> -->
                    <span>📦&nbsp;&nbsp;分类</span>
                </a>
            </li>
            <li >
                <a href="/blog/tags">
                    <!-- <i class="iconfont icon-biaoqian1"></i> -->
                    <span>📌&nbsp;&nbsp;标签</span>
                </a>
            </li>
            <li >
                <a href="/blog/archives">
                    <!-- <i class="iconfont icon-guidang"></i> -->
                    <span>📅&nbsp;&nbsp;时间</span>
                </a>
            </li>
            
            <li>
                <a id="search">
                    <!-- <i class="iconfont icon-sousuo1"></i> -->
                    <span>🔍&nbsp;&nbsp;搜索</span>
                </a>
            </li>
            
        </ul>
    </div>
</div>


<div class="search-field" id="search-field">
    <div class="search-container">
        <div class="search-input">
            <span id="esc-search"> <i class="icon-fanhui iconfont"></i></span>
            <input id="search-input" />
            <span id="begin-search">搜索</span>
        </div>
        <div class="search-result-container" id="search-result-container">

        </div>
    </div>
</div>
        <div class="index-about-mobile">
            <i> 🎶 生命因你而火热 💯 </i>
        </div>
    </div>
    
    <div class="index-middle">
        <!-- Main Content -->
        <div class="post-container">
    <div class="post-title">
        杂谈：浙江大学-数据挖掘课程-复习笔记
    </div>

    <div class="post-meta">
        <!-- <span
            class="attr">发布于：<span>2017-11-10 01:04:00</span></span> -->
        <span class="attr">发布于：<span>2017-11-10</span></span>
        <span class="attr"><a class="tag" href="/blog/author/#Gossip"
                title="Gossip">Gossip</a></span>
        
        <span class="attr">/
            
            <a class="tag" href="/blog/tags/#Course Notes" title="Course Notes">Course Notes</a>
            <span>/</span>
            
            <a class="tag" href="/blog/tags/#Data Mining" title="Data Mining">Data Mining</a>
            <span>/</span>
            
            
        </span>
        <span class="attr">访问：<span id="busuanzi_value_page_pv"></span>
        </span>
        </span>
    </div>
    <div class="post-content ">
        <h2 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1. 介绍"></a>1. 介绍</h2><h3 id="什么是数据挖掘：抽取-interesting-pattern"><a href="#什么是数据挖掘：抽取-interesting-pattern" class="headerlink" title="什么是数据挖掘：抽取 interesting pattern"></a>什么是数据挖掘：抽取 interesting pattern</h3><h3 id="数据挖掘的过程：knowledge-discovery-过程-KDD"><a href="#数据挖掘的过程：knowledge-discovery-过程-KDD" class="headerlink" title="数据挖掘的过程：knowledge discovery 过程 KDD"></a>数据挖掘的过程：knowledge discovery 过程 KDD</h3><p><img src="https://jackie-image.oss-cn-hangzhou.aliyuncs.com/17-11-13/10999007.jpg" alt></p>
<h3 id="可以被挖掘的-pattern"><a href="#可以被挖掘的-pattern" class="headerlink" title="可以被挖掘的 pattern"></a>可以被挖掘的 pattern</h3><ol>
<li><p>generalization(概括)</p>
<ul>
<li>Information integration 信息聚合，数据仓库的构建（数据清洗、变换、聚合和多维数据模型）</li>
<li>Data cube technology 数据立方技术</li>
<li>Multidimensional concept description 多维概念描述（分类和识别）</li>
</ul>
</li>
<li><p>association and correlation analysis(关联分析和相关分析)</p>
<ul>
<li>发掘 Frequent pattern</li>
<li>association correlation vs causality（关联，相关和因果关系）</li>
</ul>
</li>
<li><p>classification(分类)</p>
<ul>
<li>建立基于训练样本的模型</li>
<li>描述，区分不同的类别</li>
<li>预测一些未知的类别标记</li>
</ul>
</li>
<li><p>cluster analysis(聚类)</p>
<ul>
<li>无监督学习（比如：不知道类别标签）</li>
<li>将结果进行分类成不同的类别</li>
<li>原则：最大化类内的相似度，并且最小化类间相似度</li>
</ul>
</li>
<li><p>outlier analysis(离群点分析)</p>
<ul>
<li>outlier(离群点)：指的是不符合数据一般表现的数据个体</li>
<li>噪音？异常？</li>
<li>方法：聚类、回归分析</li>
</ul>
</li>
<li><p>Time and Ordering: Sequential Pattern, Trend and Evolution Analysis（时序数据，趋势分析和演变分析）</p>
<ul>
<li>Sequence, trend and evolution analysis（序列，趋势和演化分析）</li>
<li>挖掘数据流</li>
</ul>
</li>
<li><p>Structure and Network Analysis(结构分析和网络分析)</p>
<ul>
<li>graph mining：图数据挖掘</li>
<li>web mining：网络数据挖掘</li>
<li>信息网络分析</li>
</ul>
</li>
</ol>
<h3 id="数据挖掘的主要问题"><a href="#数据挖掘的主要问题" class="headerlink" title="数据挖掘的主要问题"></a>数据挖掘的主要问题</h3><ol>
<li>挖掘方法<ul>
<li>挖掘多种不同的知识</li>
<li>在多维空间中挖掘知识</li>
<li>跨学科</li>
<li>提高在网络环境中挖掘数据的能力</li>
<li>噪声、不确定性、数据的不完整性</li>
<li>pattern 演变</li>
<li>有约束条件的挖掘</li>
</ul>
</li>
<li>用户交互（和领域背景知识的结合）</li>
<li>可视化（Efficiency and Scalability 高效、可扩展）</li>
<li>数据类型的多变性</li>
<li>数据挖掘与社会影响</li>
</ol>
<h2 id="2-数据"><a href="#2-数据" class="headerlink" title="2. 数据"></a>2. 数据</h2><h3 id="数据对象和属性类型"><a href="#数据对象和属性类型" class="headerlink" title="数据对象和属性类型"></a>数据对象和属性类型</h3><ol>
<li>数据集的类型<ul>
<li>记录（record）：关系记录，矩阵，文档数据，交易数据</li>
<li>图和网络（graph and network）</li>
<li>有序数据（ordered）：视频、时序数据、基因序列数据</li>
<li>空间、图像和多媒体</li>
</ul>
</li>
<li>结构化数据的重要特征：<ul>
<li>维度（dimensionality）</li>
<li>稀疏度（sparsity)</li>
<li>分辨率（resolution）</li>
<li>分布（distribution）</li>
</ul>
</li>
<li>数据对象<ul>
<li>一个数据对象代表一个实体</li>
<li>也被叫做 samples, examples, instances, data points, objects, tuples</li>
<li>数据对象用属性来表述</li>
<li>rows：数据对象；columns：属性</li>
</ul>
</li>
<li>属性（Attribute or dimensions, features, variables）<ul>
<li>nominal：枚举属性（类别数据），类别，状态，是可数的，比如 Hair_color = {auburn, black, blond, brown, grey, red, white}</li>
<li>ordinal：序数属性（有序数据），属性值有一个有意义的顺序，但相邻两级之间的差距是未知的</li>
<li>binary：二元属性<ul>
<li>对称二元属性（等价，同权，比如男女）</li>
<li>非对称二元属性（不等价，如艾滋病毒的阴性和阳性，将重要的（往往是稀有的）编码为 1）</li>
</ul>
</li>
<li>numeric：数值属性<ul>
<li>数值（quantity）</li>
<li>区间属性（interval）：用相等的单位尺度的单元来表示，而且值是有序的。没有绝对的零值，并没有倍数关系（比如不能说 10℃ 是 5℃ 的两倍温暖）</li>
<li>比率属性（ratio）：有零值</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="数据的基本统计描述"><a href="#数据的基本统计描述" class="headerlink" title="数据的基本统计描述"></a>数据的基本统计描述</h3><ul>
<li><p>中位数，最大值，最小值，分位数，离群值，方差等等（median, max, min, quantiles, outliers, variance, etc.）</p>
<ul>
<li>mean，均值（代数意义上的）</li>
<li>mode，众数，可能有多个众数</li>
<li>median，中位数</li>
</ul>
</li>
<li><p>对称的和倾斜的数据：对称，正倾斜（众数小于中位数），负倾斜（众数大于中位数）</p>
<p><img src="https://jackie-image.oss-cn-hangzhou.aliyuncs.com/17-11-13/67206708.jpg" alt></p>
</li>
<li><p>分位数，离群值和盒须图</p>
<ul>
<li>分位数（Quartiles）：Q1（25 分位数），Q3（75 分位数）</li>
<li>四分位数间距（inter-quartile range），IQR=Q3-Q1</li>
<li>盒须图的五个点：min, Q1, median, Q3, max</li>
</ul>
</li>
<li><p>方差和标准差</p>
<ul>
<li><p>有偏估计方差</p>
<p><img src="https://jackie-image.oss-cn-hangzhou.aliyuncs.com/17-11-13/19846478.jpg" alt></p>
</li>
<li><p>无偏估计方差</p>
<p><img src="https://jackie-image.oss-cn-hangzhou.aliyuncs.com/17-11-13/91244837.jpg" alt></p>
</li>
</ul>
</li>
<li><p>可视化</p>
<ul>
<li><p>盒须图</p>
</li>
<li><p>统计直方图</p>
</li>
<li><p>分位数图（Quantile Plot），横轴是百分比，纵轴是数值</p>
<p><img src="https://jackie-image.oss-cn-hangzhou.aliyuncs.com/17-11-13/49705672.jpg" alt></p>
</li>
<li><p>Q-Q Plot，比较两组数据是否来自同一分布</p>
<p><img src="https://jackie-image.oss-cn-hangzhou.aliyuncs.com/17-11-13/65227165.jpg" alt></p>
</li>
<li><p>散点图（Scatter plot）</p>
</li>
</ul>
</li>
</ul>
<h3 id="数据相似度和相异度"><a href="#数据相似度和相异度" class="headerlink" title="数据相似度和相异度"></a>数据相似度和相异度</h3><ul>
<li><p>数据矩阵：n*p 矩阵，n 是数据对象个数，p 是属性个数。</p>
</li>
<li><p>相异度矩阵：n*n 矩阵</p>
</li>
<li><p>枚举属性（nominal attribute）的相异度度量：</p>
<ul>
<li>简单的匹配，相异度 d(i,j)=(p-m)/p，p 是属性个数，m 是匹配的属性</li>
<li>将枚举属性转换成二元属性（比如 color={red, green, blue}，可以转换成三个二元属性），用二元属性的相异度度量</li>
</ul>
</li>
<li><p>二元属性（binary attribute）的相异度度量：</p>
<p><img src="https://jackie-image.oss-cn-hangzhou.aliyuncs.com/17-11-13/23023651.jpg" alt></p>
<ul>
<li>对称属性的距离：$d=(r+s)/(q+r+s+t)$</li>
<li>非对称属性的距离：$d=(r+s)/(q+r+s)$</li>
<li>Jaccard 相关系数（非对称属性的相似度度量）：$sim=(q)/(q+r+s)$</li>
</ul>
</li>
<li><p>数值属性（numeric)：</p>
<ul>
<li><p>标准化：</p>
<ul>
<li><p>Z-score: $z=(x-\mu)/\sigma$，$\mu$是平均值，$\sigma$是标准差</p>
</li>
<li><p>平均绝对离差（mean absolute deviation）：计算每个属性的平均值，以及每个属性的标准差，再进行 z-score 标准化，更鲁棒</p>
<p><img src="https://jackie-image.oss-cn-hangzhou.aliyuncs.com/17-11-13/33167128.jpg" alt></p>
</li>
</ul>
</li>
<li><p>欧几里得距离（Euclidean Distance）：$d=\sqrt{(x_{i1}-x_{j1})^2+(x_{i2}-x_{j2})^2+…+(x_{ip}-x_{jp})^2}$</p>
</li>
<li><p>曼哈顿距离$d=|x_{i1}-x_{j1}|+|x_{i2}-x_{j2}|+…+|x_{ip}-x_{jp}|$</p>
</li>
<li><p>闵可夫斯基距离（Minkowski distance）：$d=\sqrt[h]{|x_{i1}-x_{j1}|^h+|x_{i2}-x_{j2}|^h+…+|x_{ip}-x_{jp}|^h}$</p>
<p>范数</p>
</li>
<li><p>上确界距离（$L_{max}$，切比雪夫距离）</p>
<p><img src="https://jackie-image.oss-cn-hangzhou.aliyuncs.com/17-11-13/9919791.jpg" alt></p>
</li>
</ul>
<p>​</p>
</li>
<li><p>有序属性（ordinal）</p>
<ul>
<li><p>标准化，映射到[0,1]，$r_{if}$是原始值的排序值，$M_f$是属性$f$的状态数</p>
<p>$z_{if}=\frac{r_{if}-1}{M_{f}-1}$</p>
</li>
<li><p>用数值属性提到的四种距离来计算</p>
</li>
</ul>
</li>
<li><p>混合类型</p>
<ul>
<li><p>将所有属性，映射到共同的区间[0,1]</p>
</li>
<li><p>计算距离：</p>
<p><img src="https://jackie-image.oss-cn-hangzhou.aliyuncs.com/17-11-13/64012434.jpg" alt></p>
<ul>
<li>指示符$\delta_{ij}=0$ 表示，对象 i 或对象 j 缺少属性 f,或者$x_{if} = x_{jf} = 0$且 f 是非对称的二元属性；否则为 1。</li>
<li>$d_{ij}^{(f)}$，表示 i 和 j 在属性 f 上的距离：<ul>
<li>二元属性或者枚举属性：相同为 0，不同为 1；</li>
<li>有序属性，用有序属性标准化的方式进行标准化</li>
<li>数值属性，两者的差/（属性 f 的最大值-属性 f 的最小值）</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>余弦相似度（Cosine Similarity），一般用于计算文档，每个文档都有一个词频向量。</p>
<p>cos(d1, d2) = (d1· d2) /||d1|| ||d2|| ，两个向量之间的余弦值，||x||是 x 的欧几里得范式</p>
</li>
</ul>
<h2 id="3-数据处理"><a href="#3-数据处理" class="headerlink" title="3. 数据处理"></a>3. 数据处理</h2><h3 id="数据质量"><a href="#数据质量" class="headerlink" title="数据质量"></a>数据质量</h3><ul>
<li><strong>准确性（accuracy）</strong></li>
<li><strong>完备性（completeness）</strong></li>
<li><strong>一致性（consistency）</strong>，有些修改了，有些没修改</li>
<li>时效性（timeliness）</li>
<li>可信性（believability）</li>
<li>可解释性（interpretability）</li>
</ul>
<h3 id="数据处理的主要任务"><a href="#数据处理的主要任务" class="headerlink" title="数据处理的主要任务"></a>数据处理的主要任务</h3><ul>
<li>数据清洗（datac cleaning）：填补缺失值，平滑噪声，去除异常值，解决不一致性</li>
<li>数据集成（data integration）：将多源数据进行集成</li>
<li>数据简化（data reduction）：降维、数量归约（使用回归，聚类等方法，用较小的表示取代数据）、数据压缩</li>
<li>数据变换和离散化（Data transformation and data discretization），进行标准化</li>
</ul>
<h3 id="数据清洗"><a href="#数据清洗" class="headerlink" title="数据清洗"></a>数据清洗</h3><ol>
<li>缺失值<ul>
<li>忽略</li>
<li>手动</li>
<li>添加为新的类别，比如 unknown</li>
<li>用平均值或者中位数来填充</li>
<li>用同一类的样本的均值或者中位数来填充</li>
<li>最有可能的值：贝叶斯形式化方法（Bayesian formula）或者决策树</li>
</ul>
</li>
<li>噪声<ul>
<li>分箱，划分成等频率的箱，用箱的均值或者中位数，或者最近边界来平滑数据</li>
<li>回归</li>
<li>聚类：检测并去除离群值</li>
<li>人机合作</li>
</ul>
</li>
<li>不一致性（如何检测？）<ul>
<li>用元数据（定义域，值域，分布等）</li>
<li>字段过载(field overloading)，用了其他属性的未使用的部分的位置</li>
<li>检查唯一性规则（每个值都应该不同），连续性规则（最低和最高之间没有确缺失值），空值规则</li>
<li>使用商业工具</li>
</ul>
</li>
<li>伪造</li>
</ol>
<h3 id="数据集成"><a href="#数据集成" class="headerlink" title="数据集成"></a>数据集成</h3><p>多源数据的结合：模式集成（schema integration， e.g. nA.cust-id = B.cust-#），个体识别（entity identification，识别有不同名称的相同的个体），检测和解决数据值冲突。</p>
<ul>
<li><p>数据集成中的冗余（redundancy）问题</p>
<ul>
<li><p>两种冗余：同一个属性或者对象有着不同的名称；可被推导出来的值</p>
</li>
<li><p>可以通过相关分析（correlation analysis）和协方差分析（covariance analysis）进行冗余检测</p>
<ul>
<li><p>相关分析：$\chi^2$卡方检验</p>
<p>$\chi^2=\sum\frac{(Observed-Expected)^2}{Expected}$</p>
<p><img src="https://jackie-image.oss-cn-hangzhou.aliyuncs.com/17-11-14/41219297.jpg" alt></p>
<p>括号中的是它的期望值，比如，90=450*300/(300+1200)，于是</p>
<p>$\chi^2=\frac{(250-90)^2}{90}+\frac{(50-120)^2}{210}+\frac{(200-360)^2}{360}+\frac{(1000-840)^2}{840}$</p>
<p>卡方越大越相关。</p>
</li>
<li><p>相关分析：皮艾森系数（Pearson’s product moment coefficient）</p>
<p><img src="https://jackie-image.oss-cn-hangzhou.aliyuncs.com/17-11-14/12962797.jpg" alt></p>
<p>​</p>
</li>
<li><p>协方差分析：针对数值型数据</p>
<p>协方差：<img src="https://jackie-image.oss-cn-hangzhou.aliyuncs.com/17-11-14/80220904.jpg" alt></p>
<p><img src="https://jackie-image.oss-cn-hangzhou.aliyuncs.com/17-11-14/48564062.jpg" alt></p>
<p>协相关系数（correlation coefficient:）：</p>
<p><img src="https://jackie-image.oss-cn-hangzhou.aliyuncs.com/17-11-14/63996666.jpg" alt></p>
<p>协方差为正，说明 A B 趋向于一起改变，A 大于期望的时候，B 也很可能大于它的期望</p>
<p>协方差为负，说明当一个属性小于它的期望，另一个则趋向于比期望更大</p>
<p>协方差为 0，说明两者独立，因为$E(A · B) = E(A)·E(B)$</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="数据简化（reduction）"><a href="#数据简化（reduction）" class="headerlink" title="数据简化（reduction）"></a>数据简化（reduction）</h3><ul>
<li><p>降低维度（Dimensionality Reduction）</p>
<ul>
<li><p>动机：维数灾难，当维度增加，数据变得稀疏，</p>
</li>
<li><p>方法：</p>
<ol>
<li><p>小波变换（wavelet transforms）</p>
<ul>
<li><p>将一个信号分解为不同频率的子带，保留数据对象之间的相对距离，只保留一小部分小波系数最强的信息，和傅立叶变换类似，但空间局部性更好，有助于保留局部细节</p>
</li>
<li><p>为什么选择小波变换</p>
<p>  有效移除离群值，多分辨率（在不同缩放率下都可以检测任意形状的聚类），高效（时间复杂度 O(N)），但只适用于低维数据</p>
</li>
</ul>
</li>
<li><p>PCA 主成分分析</p>
<p> 找出 k 个最能代表数据的 n 维正交向量（k&lt;=n），也就是找到一个投影能够捕捉到数据中最主要的变换。</p>
<ul>
<li>先标准化输入数据，使得所有属性都投影到同一区间。</li>
<li>计算 K 个标准正交向量，这些向量作为规范化输入数据的基，称为主成分。输入数据即为主成分的线性组合</li>
<li>对于主成分，按照重要程度或者强度进行排序</li>
<li>去掉排序靠后的，不重要的，方差较小的那些正交向量</li>
</ul>
</li>
<li><p>属性子集选择（attribute subset selection）</p>
<p> 通过删除不相关或者冗余的属性来减少数据量。</p>
<p> 启发式搜索（贪心算法），属性的好坏，可以用统计显著性检验来确定</p>
<ul>
<li>逐步选择：每次从属性集里选出一个最好的属性，添加到目标集合中</li>
<li>逐步删除：每次从属性集中删除一个最差的属性</li>
<li><p>两者结合：每次都选出一个最好属性，并删除一个最差的</p>
<p>​</p>
</li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
<li><p>简化数量（Numerosity Reduction）</p>
<ul>
<li><p>参数化方法</p>
<p>假设数据会符合某些模型，这样就可以只记录模型参数，忽略数据（x，y 表示数值属性）</p>
<ul>
<li>线性回归：简单直线（$y=wx+b$）</li>
<li>多元回归：用多个自变量的线性函数对因变量 Y 进行建模（$y=b_0+b_1x_1+b_2x_2+…+b_kx_k$）</li>
<li>对数线性模型：对于离散属性值，可以用对数线性模型，基于维组合的一个较小子集，估计多维空间中每个点的概率。</li>
</ul>
</li>
<li><p>非参数化方法</p>
<p>未假设模型的存在</p>
<ul>
<li>直方图：等宽分割（宽度接近）和等频分割（高度接近）</li>
<li>聚类</li>
<li>采样<ol>
<li>无放回简单随机采样</li>
<li>有放回简单随机采样</li>
<li>分层抽样（stratified sampleing）：分割数据集。对倾斜数据比较有效</li>
</ol>
</li>
<li>数据立方聚集</li>
</ul>
</li>
</ul>
</li>
<li><p>数据压缩（Data Compression）</p>
<ul>
<li>字符串压缩</li>
<li>音频/视频压缩</li>
</ul>
</li>
</ul>
<h3 id="数据变换和数据离散化"><a href="#数据变换和数据离散化" class="headerlink" title="数据变换和数据离散化"></a>数据变换和数据离散化</h3><ul>
<li><p>数据变换</p>
<ol>
<li><p>光滑（去除噪声）</p>
</li>
<li><p>属性构造（ 由已有的属性构造出新属性添加到属性集中）</p>
</li>
<li><p>聚集（汇总）</p>
</li>
<li><p>规范化（标准化）</p>
<ul>
<li><p>min-max，标准化到[new_min, new_max]</p>
<p>  $v’=\frac{v-min}{max-min}*(new_max-new_min)+new_min$</p>
</li>
<li><p>z-score</p>
<p>  $v’=\frac{v-\mu}{\sigma}$</p>
</li>
<li><p>小数定标 decimal scaling</p>
<p>  $v’=\frac{v}{10^j}$，其中 j 是使得 v’最大绝对值小于 1 的最小的整数</p>
</li>
</ul>
</li>
<li><p>离散化</p>
</li>
</ol>
</li>
<li><p>离散化</p>
<ol>
<li>分箱：无监督，自顶向下分裂，指定箱的个数；容易受离群值影响；有等宽和等深频</li>
<li>直方图：无监督，自顶向下分裂，等宽/等频</li>
<li>聚类：无监督，自顶向下分裂/自下向上合并</li>
<li>决策树：有监督，自顶向下分裂。</li>
<li>相关性分析。有监督，自下向上合并</li>
</ol>
<ul>
<li><p>概念分层</p>
<ul>
<li><p>通过用户或专家，显式的说明部分或者所有的属性层次序列</p>
</li>
<li><p>通过显示数据分组，说明分层结构的一部分，比如定义{浙江，江苏，福建}属于华东地区</p>
</li>
<li><p>自动根据每个属性的不同值个数产生概念分层</p>
<p>​</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="4-数据仓库和联机分析处理"><a href="#4-数据仓库和联机分析处理" class="headerlink" title="4. 数据仓库和联机分析处理"></a>4. 数据仓库和联机分析处理</h2><h2 id="5-数据立方技术"><a href="#5-数据立方技术" class="headerlink" title="5. 数据立方技术"></a>5. 数据立方技术</h2><h2 id="6-挖掘频繁模式、关联和相关性"><a href="#6-挖掘频繁模式、关联和相关性" class="headerlink" title="6. 挖掘频繁模式、关联和相关性"></a>6. 挖掘频繁模式、关联和相关性</h2><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><ol>
<li><p>动机：找到数据的内在规律</p>
</li>
<li><p>项集（itemset）</p>
</li>
<li><p>事务（transaction），为一个非空项集</p>
</li>
<li><p>频度（frequency），</p>
</li>
<li><p>关联规则（association rules），X=&gt;Y，X，Y 是两个不相交的非空项集。</p>
</li>
<li><p>强关联规则：支持度和置信度都高于阈值</p>
</li>
<li><p>支持度（support）：包含$X \cup Y$的事务的出现概率</p>
</li>
<li><p>置信度（confidence）：包含 X 的事务同时也包含 Y 的概率，P(Y|X)</p>
<p> <img src="https://jackie-image.oss-cn-hangzhou.aliyuncs.com/17-11-15/1487277.jpg" alt></p>
<p> $support(Beer \Rightarrow Diaper)=count(10,20,30) / 5 = 60\%, confidence(Beer \Rightarrow Diaper)=count(10,20,30) / count(10,20,30) = 100\%$</p>
<p> $support(Diaper \Rightarrow Beer)=count(10,20,30) / 5 = 60\%, confidence(Diaper \Rightarrow Beer)=count(10,20,30) / count(10,20,30,50) = 75\%$</p>
</li>
<li><p>因为长项集的子项集组合过多，比如包含 100 项的相机，它的子集组合就有$2^{100}-1$个。故而把问题转换成挖掘其中的闭频繁项集和极大频繁项集：</p>
<ul>
<li>closed pattern：如果不存在 X 的真超项集 Y，使得 Y 和 X 在数据集 D 中有着相同的频度，那么称 X 为闭的（closed）</li>
<li>Max-Patterns：如果 X 是频繁的，且不存在 X 的超项集 Y，并且 Y 是频繁的</li>
</ul>
</li>
<li><p>时间复杂度：最坏情况$M^N$，M 是不同的项个数，N 是事务的最大长度</p>
</li>
</ol>
<h2 id><a href="#" class="headerlink" title="#"></a>#</h2><h3 id="Apriori-算法："><a href="#Apriori-算法：" class="headerlink" title="Apriori 算法："></a>Apriori 算法：</h3><p>基于一个先验性质，频繁项集的所有非空子集也一定是频繁的，反而言之，如果一个项集是不频繁的，那么它的任何超集也都不用再进行验证。</p>
<ul>
<li>第一次生成一项集，排除里面支持度小于阈值的项集。</li>
<li>根据上一次生成的项集，形成 N+1 项集</li>
<li>排除 N+1 项集中，支持度小于阈值的项集，重复上一步，直到所有项集的支持度都低于阈值</li>
</ul>
<p>案例：</p>
<p><img src="https://jackie-image.oss-cn-hangzhou.aliyuncs.com/17-11-15/11954617.jpg" alt></p>
<h3 id="提高-Apriori-算法："><a href="#提高-Apriori-算法：" class="headerlink" title="提高 Apriori 算法："></a>提高 Apriori 算法：</h3><ul>
<li>问题：多次扫描数据库中的事务，庞杂的候选项，计数候选项支持度的开销大</li>
<li>解决：<ol>
<li>划分（partition）：只需要两次扫描数据库。第一次，将数据库 D 中的事务，划分成 n 个非重叠分区，计算每个分区的局部最小支持度计数阈值（min_sup * 分区事务个数）。若项集超过这个局部最小支持度计数，那么认为这个项集是局部频繁项集。全局的频繁项集一定出现在局部频繁项集中，故而将局部频繁项集作为 D 的候选项集；第二次，再次扫描 D，评估候选项集的实际支持度，删除低于阈值的项集。</li>
<li>散列（hash，DHP）：利用散列哈希，如果哈希结束后，桶中的项集个数比阈值还小，那么这个桶中的项集就一定会被淘汰。而桶中项集个数大于阈值，也不一定就是频繁项集。</li>
<li>采样（sampling）：牺牲精度换取可行性。利用 D 的一个采样 S，找出 S 中的频繁项集（故而阈值也会重新计算，此时的频繁是相对 S 的频繁），但 S 中的频繁项集并不一定是 D 中的频繁项集，会有丢失。故而要用低于最小支持度的阈值来搜索 S，从而提高精度。</li>
<li>动态项集计数（DIC）</li>
</ol>
</li>
</ul>
<h3 id="模式增长方法（Pattern-Growth-Approach）"><a href="#模式增长方法（Pattern-Growth-Approach）" class="headerlink" title="模式增长方法（Pattern-Growth Approach）"></a>模式增长方法（Pattern-Growth Approach）</h3><ol>
<li><p>构建 fp tree：</p>
<p> <img src="https://jackie-image.oss-cn-hangzhou.aliyuncs.com/17-11-15/53320418.jpg" alt></p>
</li>
<li><p>条件模式基（conditional pattern bases）</p>
<p> <img src="https://jackie-image.oss-cn-hangzhou.aliyuncs.com/17-11-15/10773567.jpg" alt></p>
</li>
<li><p>寻找条件模式树（conditional FP-tree，类似于寻找最长公共子序列）</p>
<p> <img src="https://jackie-image.oss-cn-hangzhou.aliyuncs.com/17-11-15/19614960.jpg" alt></p>
</li>
<li><p>简化，前缀可以被简化成一个节点</p>
<p> <img src="https://jackie-image.oss-cn-hangzhou.aliyuncs.com/17-11-15/95693211.jpg" alt></p>
</li>
<li><p>fp-tree 的优势</p>
<ul>
<li>完全性（completeness）：包含了 fp mining 需要的所有信息，不拆分任何一个事务的长 pattern</li>
<li>紧密型（compactness）：去除了无关信息（非频繁的项被省去）；高频项被放在前面；不会比初始数据库更大</li>
</ul>
</li>
<li><p>挖掘方法</p>
<ul>
<li>对每个频繁项，构造它的条件模式基（conditional pattern-base），进而构造它的条件频繁模式树（conditional FP-tree）</li>
<li>对每个 conditional FP-tree，重复以上步骤</li>
<li>直到 FP-tree 是空的，或者只有一条路径（单一路径的所有子路径，组成了频繁模式）</li>
</ul>
</li>
<li><p>分割投影</p>
<p> 为了让 fp-tree 能放进主存，需要将数据库划分成投影数据库的集合。比如 4 图中，就可以先分成两个以 r1 为前缀项集的投影数据库$\{b1\}，\{ \{c1,c2\},\{c1,c3\}\}$</p>
</li>
</ol>
<h3 id="用等价类变换（ECLAT）进行垂直数据格式挖掘"><a href="#用等价类变换（ECLAT）进行垂直数据格式挖掘" class="headerlink" title="用等价类变换（ECLAT）进行垂直数据格式挖掘"></a>用等价类变换（ECLAT）进行垂直数据格式挖掘</h3><p>前面的方法都是 TID 项集格式的挖掘方式，这种数据格式称为水平数据格式；而垂直数据格式刚好是它的一个转置。$t(X)= {T1,T2,T3},  t(XY) = {T1, T3}$</p>
<p>加速：比如上方，$Diffset(XY,X)={T2}$，这样就不用记录$t(XY)$的两个项，而只要存储差集的一个项就行了。</p>
<h3 id="挖掘闭频繁模式和极大模式"><a href="#挖掘闭频繁模式和极大模式" class="headerlink" title="挖掘闭频繁模式和极大模式"></a>挖掘闭频繁模式和极大模式</h3><ol>
<li><p>挖掘闭模式</p>
<ul>
<li><strong>项合并</strong>：如果包含频繁项集$X$的事务都包含$Y$，且不包含$Y$的任何真超集，那么$X \cup Y$形成一个闭频繁项集，并且可以不再搜索包含$X$且不包含$Y$的任何项集。</li>
<li><strong>子项集剪枝</strong>：如果 X 是一个已发现的闭频繁项集 Y 的真子集，且$support_count(X)=support_count(Y)$(说明，X 没有单独出现在任何事务中)，那么 X 和 X 的子集都不可能是闭频繁项集</li>
<li><strong>项跳过</strong>：在头表不同层，某个局部频繁项都有着一样的 support，那么这一项就可以从头表中删除</li>
</ul>
</li>
<li><p>挖掘极大模式</p>
<p> <img src="https://jackie-image.oss-cn-hangzhou.aliyuncs.com/17-11-15/71303397.jpg" alt></p>
</li>
</ol>
<h3 id="找出-interesting-pattern"><a href="#找出-interesting-pattern" class="headerlink" title="找出 interesting pattern"></a>找出 interesting pattern</h3><p>强关联规则并不一定准确。需要其他度量</p>
<ul>
<li><p>correlations（相关性）。</p>
<ul>
<li><p>提升度 lift</p>
<p>假如 A 项集和 B 项集的出现是独立事件，那么$P(A \cup B) = P(A)P(B)$；否则，两者相互依赖（dependent）和相关（correlated），可以通过提升度（lift）来表示</p>
<p>$lift(A, B) = \frac{P(A \cup B)}{P(A)P(B)}=\frac{P(B|A)}{P(B)}$</p>
<p>如果提升度小于 1，说明发生 A 时发生 B 的概率，比光是发生 B 的概率要小，A 和 B 负相关；</p>
<p>如果提升度大于 1，说明发生 A 时发生 B 的概率，比光是发生 B 的概率还大，A 和 B 正相关；</p>
<p>如果提升度等于 1，说明发生 A 时发生 B 的概率，和光是发生 B 的概率一致，说明两者无关独立。</p>
</li>
<li><p>卡方$\chi^2$</p>
</li>
</ul>
</li>
<li><p>不平衡比（Imbalance Ratio）</p>
<p>$IR(A, B) = \frac{|sup(A)-sup(B)|}{sup(A)+sup(B)-sup(A \cup B)}$</p>
<p>分子是支持度之差的绝对值；分母是包含 A 或 B 的事务个数。越大越不平衡。</p>
</li>
</ul>
<h2 id="7-高级模式挖掘"><a href="#7-高级模式挖掘" class="headerlink" title="7. 高级模式挖掘"></a>7. 高级模式挖掘</h2><h2 id="8-分类"><a href="#8-分类" class="headerlink" title="8. 分类"></a>8. 分类</h2><p>预测问题包括分类和数值预测。</p>
<h3 id="分类：一个两步过程"><a href="#分类：一个两步过程" class="headerlink" title="分类：一个两步过程"></a>分类：一个两步过程</h3><ol>
<li>模型建构<ul>
<li>每个样本都假设属于一个预定义的类</li>
<li>模型可能表现为：分类规则，决策树或者数学公式</li>
</ul>
</li>
<li>模型使用<ul>
<li>评价模型精确程度</li>
<li>假如精确度可接受，那么就可以用来标记新数据</li>
<li>监督学习和无监督学习<ul>
<li>监督学习：训练数据是有标记的</li>
<li>无监督学习：训练数据无标记，目标是进行聚类</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="决策树归纳"><a href="#决策树归纳" class="headerlink" title="决策树归纳"></a>决策树归纳</h3><ul>
<li><p>决策树的构建算法：自顶向下的递归分治算法</p>
<ol>
<li>一开始所有训练样本都在根节点上，所有的属性都是有类别的（假如是连续的，需要提前离散化）</li>
<li>基于参数中给定的分裂准则，用选定的属性对样本进行划分，不断迭代</li>
<li>直到满足以下任一条件：<ul>
<li>给定节点中的所有样本都是同一类的</li>
<li>没有剩余的属性可以被用来做进一步分割</li>
<li>没有剩余的样本了</li>
</ul>
</li>
</ol>
</li>
<li><p>决策树构建中的分裂准则</p>
<ol>
<li><p>信息增益（Information Gain)</p>
<p> 选择具有最高信息增益的属性作为节点 N 的分裂属性</p>
<ul>
<li><p>对 D 中的元组进行分类所需要的期望信息，也被称为 D 的熵：</p>
<p>  $Info(D)=-\sum p_ilog_2(p_i)$</p>
<p>  $p_i$是$D$中任意元组属于类$C_i$的概率（非 0）</p>
</li>
<li><p>利用某个属性对 D 进行分区，得到的分区不一定是准确的分类，所以需要计算，要得到准确的分类，我们还需要多少信息：</p>
<p>  $Info_A(D)=\sum \frac{|D_j|}{|D|} \times Info(D_j)$</p>
<p>  其中$\frac{|D_j|}{|D|}$充当第 j 个分区的权重。$Info_A(D)$是基于 A 划分 D 所需要的期望信息，所需的期望信息越小，分区的纯度越高。</p>
</li>
<li><p>信息增益：</p>
<p>  $Gain(A）=Info(D)-Info_A(D)$</p>
<p>  选择最高信息增益的属性作为分裂属性，也就是说选择$Info_A(D)$最小。</p>
</li>
<li><p>计算连续值得的信息增益</p>
<p>  A 的值进行递增序排序，每对相邻的中值作为一个可能的分裂点（$(a_i+a_{i+1})/2$），对于 A 的给定的 v 个值，则需要计算 v-1 个可能的划分。</p>
<p>  对每个分裂点计算$Info_A(D)$，对每个分裂点，分区个数是 2，选出最小期望信息需求的点作为分裂点。</p>
</li>
</ul>
</li>
<li><p>增益率</p>
<p> $GainRate(A) = \frac{Gain(A)}{splitInfo_A(D)}$</p>
<p> 其中，</p>
<p> $splitInfo_A(D)=-\sum \frac{|D_j|}{|D|} \times log_2(\frac{|D_j|}{|D|})$</p>
</li>
<li><p>基尼指数(Gini index)，针对二元分裂</p>
<ul>
<li><p>基尼指数，度量 D 的数据分区的不纯度：</p>
<p>  $Gini(D) = 1 - \sum p_i^2$</p>
</li>
<li><p>利用属性 A，将 D 划分为两个分区，从而得到的基尼指数：</p>
<p>  $Gini_A(D)=\frac{|D_1}{|D|}Gini(D_1)+\frac{|D_2}{|D|}Gini(D_2)$</p>
</li>
<li><p>基尼指数下降：</p>
<p>  $\Delta Gini(A)=Gini(D)-Gini_A(D)$</p>
</li>
</ul>
</li>
</ol>
</li>
<li><p>过拟合和剪枝</p>
<ul>
<li>因为噪声跟离群点的关系，有许多分支反映了训练数据中的一场，需要进行剪枝来处理这种过拟合的问题。</li>
<li>先剪枝和后剪枝<ul>
<li>先剪枝（prepruning），通过提前停止树的创建来剪枝</li>
<li>后剪枝（postpruning），删除节点的分支而用叶节点代替</li>
</ul>
</li>
</ul>
</li>
<li><p>大数据库的分类</p>
<ul>
<li><p>可伸缩的决策树算法，RainForest：</p>
<ul>
<li><p>AVG-set：在每个节点上，对每个属性都维护一个 AVC-set。</p>
</li>
<li><p>AVC-group：节点上的所有 AVC-set 的集合。</p>
<p><img src="https://jackie-image.oss-cn-hangzhou.aliyuncs.com/17-11-16/54708839.jpg" alt></p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="贝叶斯分类方法"><a href="#贝叶斯分类方法" class="headerlink" title="贝叶斯分类方法"></a>贝叶斯分类方法</h3><ol>
<li><p>贝叶斯定理</p>
<ul>
<li><p>先验概率，$P(H)$是 H 的先验概率</p>
</li>
<li><p>后验概率：$P(H|X)$是在条件 X 下，H 的后验概率</p>
</li>
<li><p>贝叶斯定理：</p>
<p>  $P(H|X)=\frac{P(X|H)P(H)}{P(X)}$</p>
</li>
</ul>
</li>
<li><p>朴素贝叶斯分类</p>
<ul>
<li><p>最大化$P(C_i|X)$：假定一个 tuple 用一个 n 维属性向量$X=\{x_1,x_2,…x_n\}$表示，且假定有 m 个类，那么配件单贝叶斯分类法中，预测$X$属于$C_i$的概率为：$P(C_i|X)$，只要找到这个最大值对应的$C_i$即可。</p>
</li>
<li><p>最大化$P(X|C_i)$：而根据贝叶斯公式，只要找到$P(X|C_i)P(C_i)$的最大值即可。加入类的先验概率未知，我们通常假设所有类的先验概率一致，于是我们只要找到$P(X|C_i)$的最大值即可</p>
</li>
<li><p>假设$X$的各个属性之间相互独立，不存在依赖关系，那么</p>
<p>  $P(X|C_i)=\prod P(x_k|C_i)$</p>
<ul>
<li><p>如果属性$x_k$是分类属性，那么概率即为训练集中属性值为$x_k$，且属于$C_i$的 tuple 在$C_i$中的比例</p>
</li>
<li><p>如果属性是连续值，一般假设属性服从高斯分布。</p>
<p>  $P(x_k|C_i)=g(x_k,\mu_{C_i},\sigma_{C_i})$<img src="https://jackie-image.oss-cn-hangzhou.aliyuncs.com/17-11-16/66390485.jpg" alt></p>
</li>
</ul>
</li>
<li><p>example</p>
<p>  <img src="https://jackie-image.oss-cn-hangzhou.aliyuncs.com/17-11-16/32057659.jpg" alt></p>
</li>
<li><p>关于 0 概率：拉普拉斯校准</p>
<p>  假设训练集很大，对每个计数都加 1，也不会对概率产生太大变化，从而避免 0 概率</p>
</li>
<li><p>优缺点</p>
<ul>
<li>优点：容易实现，在大部分情况下结果不错</li>
<li>缺点：基于分类条件独立假设，可以用贝叶斯信任网络来解决这个问题</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="模型评估与选择"><a href="#模型评估与选择" class="headerlink" title="模型评估与选择"></a>模型评估与选择</h3><ol>
<li><p>混淆矩阵：对于给定 m 个类，混淆矩阵至少是一个 m*m 的表。以下是一个 2*2 的混淆矩阵，纵向是实际分类，横向是预测分类。</p>
<p> <img src="https://jackie-image.oss-cn-hangzhou.aliyuncs.com/17-11-16/68799474.jpg" alt></p>
<ul>
<li>准确率：$accuracy=(TP+TN)/(P+N)$</li>
<li>错误率：$error rate=(FP+FN)/(P+N)$</li>
<li>有些数据是不平衡的，比如在癌症检测，显然 cancer=yes 的元组才是我们关注的，于是有了以下两个度量：<ul>
<li>灵敏性（正确识别的正元组的比例）：$sensitivity=TP/P$，反映了识别正例的能力</li>
<li>特效性（正确识别的负元组的比例）：$specificity=TN/N$，反映了识别反例的能力</li>
</ul>
</li>
<li>精度（正确识别的正元组在预测为正元组中的比例）：$precision=(TP)/(TP+FP)$</li>
<li>召回率：$recall=TP/P$，其实也就是灵敏性</li>
<li>$F$度量：$F=(2 \times precision \times recall)/(precision+recall)$</li>
<li>$F_\beta$度量：$F_\beta=((1+\beta^2) \times precision \times recall)/(\beta^2 \times precision+recall)$</li>
</ul>
</li>
<li><p>保持方法和随机二次抽样</p>
<ul>
<li>保持方法（holdout）：将数据随机的分成训练集跟检验集（通常 2/3 作为训练集）</li>
<li>随机二次抽样（random subsampling）：将保持方法重复 k 次，结果取平均值。</li>
</ul>
</li>
<li><p>交叉验证（cross-validation）</p>
<ul>
<li>k 折交叉验证（k-fold cross-validation），将数据随机分成 k 个相互不相交的子集（折），进行 k 次训练和检验。其中第 i 次迭代，用分区 i 作为检验集而用其余的作为训练集。准确率计算是用 k 次迭代的总数进行计算。</li>
</ul>
</li>
<li><p>自助法（bootstrap）</p>
<p> 在小数据集下比较好。</p>
<ul>
<li><p>$.632$自助法：对于给定的包含 d 个元组的数据集，有放回抽样 d 次，产生 d 个样本的自主样本集或训练集，其余作为验证。平均情况下，63.2%的数据会被用于训练。</p>
<p>  准确率计算：</p>
<p>  $Acc(M)=\sum(0.632 \times Acc(M_i)_{test_set} + 0.368 \times Acc(M_i)_{train_set})$</p>
<p>  $Acc(M_i)_{test_set}$是对于检验集 i 的准确率，$Acc(M_i)_{train_set})$是对于源数据的准确率</p>
</li>
</ul>
</li>
<li><p>选择模型的标准：</p>
<ul>
<li>准确率</li>
<li>速度</li>
<li>鲁棒性</li>
<li>可扩展性（对于大数据库的高效性）</li>
<li>可解释性</li>
</ul>
</li>
</ol>
<h3 id="提高分类准确率"><a href="#提高分类准确率" class="headerlink" title="提高分类准确率"></a>提高分类准确率</h3><ol>
<li>装袋（bagging）：对于不同的训练集 Di（每个训练集都是一个自助样本）训练的分类模型 Mi。为了对一个未知元组 X 进行分类，每个分类器 Mi 都会返回它的预测结果，算作投票中的一票，统计最终的票，将最高的得票赋予 X。</li>
<li>提升（boosting）：迭代学习。初始所有训练集的元组权重都一致，每一轮迭代，提升上一次测试中出错的元组的权重，降低正确的元组的权重。</li>
<li>随机森林（random forest）</li>
</ol>
<h2 id="9-高级分类方法"><a href="#9-高级分类方法" class="headerlink" title="9. 高级分类方法"></a>9. 高级分类方法</h2><h3 id="惰性学习法"><a href="#惰性学习法" class="headerlink" title="惰性学习法"></a>惰性学习法</h3><ul>
<li><p>k-最近邻分类</p>
<p>​</p>
</li>
</ul>
<h2 id="10-聚类"><a href="#10-聚类" class="headerlink" title="10. 聚类"></a>10. 聚类</h2><h3 id="聚类质量"><a href="#聚类质量" class="headerlink" title="聚类质量"></a>聚类质量</h3><ol>
<li>高类内相似度，低类外相似度</li>
<li>聚类质量依赖于：相似度度量；聚类的实现；能够发掘隐藏 pattern 的能力</li>
<li>聚类质量的度量方法：相异度/相似度矩阵</li>
<li>聚类分析需要考量的因素：<ul>
<li>划分准则：单层划分和多层划分（互相之间有层级关系）</li>
<li>簇的分离性：互斥（一个客户只能属于一个组）和不互斥（一个文档可能有多个主题）</li>
<li>相似度度量：基于距离和基于连通性</li>
<li>聚类空间</li>
<li>可伸缩性</li>
<li>处理不同类型属性的能力</li>
<li>有约束条件的聚类</li>
</ul>
</li>
</ol>
<h3 id="主要聚类方法"><a href="#主要聚类方法" class="headerlink" title="主要聚类方法"></a>主要聚类方法</h3><ol>
<li><p>划分方法</p>
<ul>
<li>将数据划分成 k 个分区，保证每个分区最少有一个对象；例如 k-means，k-medoids，CLARANS</li>
<li>发现球形互斥的簇</li>
<li>对中小规模数据集有效</li>
</ul>
</li>
<li><p>层次方法</p>
<ul>
<li>凝聚或者分裂的方法。层次聚类方法可以是基于距离或者密度和连通性的。</li>
<li>无法纠正错误的合并或划分</li>
</ul>
</li>
<li><p>基于密度的方法</p>
<p> 基于对象之间的距离进行聚类，只能发现球状簇。主要思想：只要“邻域”中的密度超过某个阈值，就继续增长。对于给定簇中的每个数据点，在给定半径的邻域中至少包含最少数目的点。</p>
</li>
<li><p>基于网格的方法</p>
<p> 用网格化的方法把对象空间量化为有限个单元。</p>
</li>
</ol>
<h3 id="划分方法"><a href="#划分方法" class="headerlink" title="划分方法"></a>划分方法</h3><p>一种度量簇质量的方法：</p>
<p><img src="https://jackie-image.oss-cn-hangzhou.aliyuncs.com/17-11-17/11929324.jpg" alt></p>
<p>$c_i$是簇$C_i$的代表（形心）</p>
<ol>
<li><p>k-means：局部最优，但不一定收敛到全局最优</p>
<p> 将簇的形心定义为簇内点的均值。</p>
<ul>
<li>初始选取 k 个点，每个点代表一个簇的初始均值或中心。</li>
<li>其余点根据欧氏距离，分配给距离最近的簇。</li>
<li>更新迭代簇内均值，再分配。</li>
<li><p>直到不再变化。</p>
<p>优点是高效；缺点是只在连续 n 维空间中有效，需要提前确定 k，对噪声和离群值敏感，无法处理非凸形状的数据</p>
</li>
</ul>
</li>
<li><p>k-medoids</p>
<p> 将簇的形心定义为簇内某个实际的点。</p>
<ul>
<li>初始选取 k 个点，每个点代表一个簇的初始均值或中心。</li>
<li>其余点根据欧氏距离，分配给距离最近的簇。</li>
<li>随机选择一个非代表对象$O_{random}$代替$O_j$，观察绝对误差标准是否降低</li>
<li>如果降低，那么说明应该进行替换，并且重新形成簇</li>
<li><p>直到不再变化</p>
<p>其中，绝对误差标准（absolute-erro criterion）的计算方法：如上。</p>
</li>
</ul>
</li>
</ol>
<h3 id="层次方法"><a href="#层次方法" class="headerlink" title="层次方法"></a>层次方法</h3><h3 id="基于密度的方法"><a href="#基于密度的方法" class="headerlink" title="基于密度的方法"></a>基于密度的方法</h3><ul>
<li><p>主要特点：</p>
<ul>
<li>可以发现任意形状的簇</li>
<li>能应对噪声</li>
<li>只扫描一遍</li>
<li>需要密度参数作为终止条件</li>
</ul>
</li>
<li><p>参数和基本概念：</p>
<ul>
<li><p>Eps：邻域的最大半径（确定领域大小）</p>
</li>
<li><p>MinPts：邻域最大半径内的最小点数量（确定邻域最大密度）</p>
</li>
<li><p>核心对象（core object）：eps 邻域内至少包含 MinPts 个对象（MinPts 由参数给定）</p>
</li>
<li><p>直接密度可达（directly density-reachable）：p 在 q 的 eps 邻域内，说明 p 是 q 直接密度可达的</p>
</li>
<li><p>密度可达的（Density-reachable）：存在对象链 p1,…,pn，后一个是前一个直接密度可达的，那么说明 pn 是 p1 密度可达的；密度可达并不是一个等价关系，只有当 p1,pn 都是核心对象时，才一定保证可逆。</p>
<p><img src="https://jackie-image.oss-cn-hangzhou.aliyuncs.com/17-11-17/31501679.jpg" alt></p>
</li>
<li><p>密度相连的（Density-connected）：存在 p1，p2，q，p1 和 q 以及 p2 和 q 都是密度可达的，那么 p1 和 p2 是密度相连的。密度相连是等价关系。</p>
<p><img src="https://jackie-image.oss-cn-hangzhou.aliyuncs.com/17-11-17/63923841.jpg" alt></p>
</li>
</ul>
</li>
<li><p>DBSCAN（Density-Based Spatial Clustering of Applications with Noise）</p>
<p>对每个核心对象，将它的所有密度可达的（但未被访问过的）对象添加到自身集合中作为它的簇。</p>
<p>未被添加的点，就是噪声</p>
</li>
</ul>
<h3 id="基于网格的方法"><a href="#基于网格的方法" class="headerlink" title="基于网格的方法"></a>基于网格的方法</h3><h3 id="聚类评估"><a href="#聚类评估" class="headerlink" title="聚类评估"></a>聚类评估</h3><ul>
<li><p>评估聚类趋势（assessing clustering tendency）</p>
<ul>
<li><p>只有对有非随机结构的数据集进行聚类，才有可能产生有意义的聚类。所以聚类要求数据的非均匀分布。</p>
</li>
<li><p>霍普金斯统计量（Hopkins Statistic）</p>
<p><img src="https://jackie-image.oss-cn-hangzhou.aliyuncs.com/17-11-17/66309480.jpg" alt></p>
</li>
</ul>
</li>
<li><p>确定簇数量（determine the number of clusters）</p>
</li>
<li><p>测定聚类质量</p>
</li>
</ul>

        
        <br />
        <div id="comment-container">
        </div>
        <div id="disqus_thread"></div>

        <div id="lv-container">
        </div>

    </div>
</div>
    </div>
    
    
    <div class="index-right">
        <div id="toc" class="toc-article">
    <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-介绍"><span class="toc-text">1. 介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#什么是数据挖掘：抽取-interesting-pattern"><span class="toc-text">什么是数据挖掘：抽取 interesting pattern</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#数据挖掘的过程：knowledge-discovery-过程-KDD"><span class="toc-text">数据挖掘的过程：knowledge discovery 过程 KDD</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#可以被挖掘的-pattern"><span class="toc-text">可以被挖掘的 pattern</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#数据挖掘的主要问题"><span class="toc-text">数据挖掘的主要问题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-数据"><span class="toc-text">2. 数据</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#数据对象和属性类型"><span class="toc-text">数据对象和属性类型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#数据的基本统计描述"><span class="toc-text">数据的基本统计描述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#数据相似度和相异度"><span class="toc-text">数据相似度和相异度</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-数据处理"><span class="toc-text">3. 数据处理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#数据质量"><span class="toc-text">数据质量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#数据处理的主要任务"><span class="toc-text">数据处理的主要任务</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#数据清洗"><span class="toc-text">数据清洗</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#数据集成"><span class="toc-text">数据集成</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#数据简化（reduction）"><span class="toc-text">数据简化（reduction）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#数据变换和数据离散化"><span class="toc-text">数据变换和数据离散化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-数据仓库和联机分析处理"><span class="toc-text">4. 数据仓库和联机分析处理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-数据立方技术"><span class="toc-text">5. 数据立方技术</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-挖掘频繁模式、关联和相关性"><span class="toc-text">6. 挖掘频繁模式、关联和相关性</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#基本概念"><span class="toc-text">基本概念</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#null"><span class="toc-text">#</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Apriori-算法："><span class="toc-text">Apriori 算法：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#提高-Apriori-算法："><span class="toc-text">提高 Apriori 算法：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#模式增长方法（Pattern-Growth-Approach）"><span class="toc-text">模式增长方法（Pattern-Growth Approach）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#用等价类变换（ECLAT）进行垂直数据格式挖掘"><span class="toc-text">用等价类变换（ECLAT）进行垂直数据格式挖掘</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#挖掘闭频繁模式和极大模式"><span class="toc-text">挖掘闭频繁模式和极大模式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#找出-interesting-pattern"><span class="toc-text">找出 interesting pattern</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-高级模式挖掘"><span class="toc-text">7. 高级模式挖掘</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-分类"><span class="toc-text">8. 分类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#分类：一个两步过程"><span class="toc-text">分类：一个两步过程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#决策树归纳"><span class="toc-text">决策树归纳</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#贝叶斯分类方法"><span class="toc-text">贝叶斯分类方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#模型评估与选择"><span class="toc-text">模型评估与选择</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#提高分类准确率"><span class="toc-text">提高分类准确率</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-高级分类方法"><span class="toc-text">9. 高级分类方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#惰性学习法"><span class="toc-text">惰性学习法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-聚类"><span class="toc-text">10. 聚类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#聚类质量"><span class="toc-text">聚类质量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#主要聚类方法"><span class="toc-text">主要聚类方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#划分方法"><span class="toc-text">划分方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#层次方法"><span class="toc-text">层次方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#基于密度的方法"><span class="toc-text">基于密度的方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#基于网格的方法"><span class="toc-text">基于网格的方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#聚类评估"><span class="toc-text">聚类评估</span></a></li></ol></li></ol>
</div>
    </div>
    
</div>

<footer class="footer">
  <ul class="list-inline text-center">
         
  </ul>
  
  <p>
    <a href="http://panjiacheng.site">@Jiacheng Pan</a> Created By
    <a href="https://hexo.io/">Hexo</a> Theme
    <a href="https://github.com/aircloud/hexo-theme-aircloud">AirCloud</a>
    <a href="https://github.com/aircloud">@Xiaotao</a>
  </p>
</footer>


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<!-- <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script> -->
<script src="https://cdn.bootcdn.net/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>

<script>
    // We expose some of the variables needed by the front end
    window.hexo_search_path = "search.json"
    window.hexo_root = "/blog/"
    window.isPost = true
</script>
<script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script>
<script src="/blog/js/index.js"></script>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




    <script type="text/javascript">
       (function() {
           if (typeof LivereTower === 'function') { return; }

           var j, d = document.getElementById('lv-container');

           d.setAttribute('data-id','city');
           d.setAttribute('data-uid' , 'MTAyMC8yOTk2MC82NTI1');

           j = document.createElement('script');
           j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
           j.async = true;

           d.appendChild(j);
       })();
    </script>
    <noscript>为正常使用来必力评论功能请激活JavaScript</noscript>
    </div>

</html>
