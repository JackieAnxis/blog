<!DOCTYPE html>
<html lang="en">
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="Jiacheng Pan&#39;s Blog">
  <meta name="keyword" content>
  <!-- <link rel="shortcut icon" href="/blog/img/favicon.ico"> -->
  <link rel="shortcut icon" href="https://avatars2.githubusercontent.com/u/19246028?s=460&amp;u=8d4f9462069e17c42c9f7e460369ac703146ceb9&amp;v=4">

  <title>
     ICCV2017：第一人称视角手势估计 - Jiacheng Pan&#39;s Blog 
  </title>

  <!-- Custom CSS -->
  <link rel="stylesheet" href="/blog/css/aircloud.css"> <link rel="stylesheet" href="/blog/css/gitment.css">
  <!--<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">-->
  <link href="//at.alicdn.com/t/font_620856_pl6z7sid89qkt9.css" rel="stylesheet" type="text/css">
  <!-- ga & ba script hoook -->
  <script></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body>

<!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="site-nav-toggle" id="site-nav-toggle">
    <button>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
    </button>
</div>

<div class="index-about">
    <i> 🎶 生命因你而火热 💯 </i>
</div>

<div class="index-container">
    
    <div class="index-left">
        <div class="nav" id="nav">
    <div class="avatar-name">
        <div class="avatar ">
            <a href="http://panjiacheng.site"><img
                    src="https://avatars2.githubusercontent.com/u/19246028?s=460&amp;u=8d4f9462069e17c42c9f7e460369ac703146ceb9&amp;v=4" /></a>
        </div>
        <div class="name">
            <a href="http://panjiacheng.site">
                <i>Jiacheng Pan</i>
            </a>
        </div>
    </div>
    <div class="contents" id="nav-content">
        <ul>
            <!-- <li >
                <a
                    href="http://panjiacheng.site">
                    <i class="iconfont icon-shouye2"></i>
                    <span>🏡&nbsp;&nbsp;主页</span>
                </a>
            </li> -->
            <li >
                <a href="/blog/">
                    <!-- <i class="iconfont icon-guidang1"></i> -->
                    <span>⌨️&nbsp;&nbsp;博客</span>
                </a>
            </li>
            <li >
                <a href="/blog/author">
                    <!-- <i class="iconfont icon-guanyu2"></i> -->
                    <span>📦&nbsp;&nbsp;分类</span>
                </a>
            </li>
            <li >
                <a href="/blog/tags">
                    <!-- <i class="iconfont icon-biaoqian1"></i> -->
                    <span>📌&nbsp;&nbsp;标签</span>
                </a>
            </li>
            <li >
                <a href="/blog/archives">
                    <!-- <i class="iconfont icon-guidang"></i> -->
                    <span>📅&nbsp;&nbsp;时间</span>
                </a>
            </li>
            
            <li>
                <a id="search">
                    <!-- <i class="iconfont icon-sousuo1"></i> -->
                    <span>🔍&nbsp;&nbsp;搜索</span>
                </a>
            </li>
            
        </ul>
    </div>
</div>


<div class="search-field" id="search-field">
    <div class="search-container">
        <div class="search-input">
            <span id="esc-search"> <i class="icon-fanhui iconfont"></i></span>
            <input id="search-input" />
            <span id="begin-search">搜索</span>
        </div>
        <div class="search-result-container" id="search-result-container">

        </div>
    </div>
</div>
        <div class="index-about-mobile">
            <i> 🎶 生命因你而火热 💯 </i>
        </div>
    </div>
    
    <div class="index-middle">
        <!-- Main Content -->
        <div class="post-container">
    <div class="post-title">
        ICCV2017：第一人称视角手势估计
    </div>

    <div class="post-meta">
        <!-- <span
            class="attr">发布于：<span>2018-05-06 00:25:00</span></span> -->
        <span class="attr">发布于：<span>2018-05-06</span></span>
        <span class="attr"><a class="tag" href="/blog/author/#PaperReading"
                title="PaperReading">PaperReading</a></span>
        
        <span class="attr">/
            
            <a class="tag" href="/blog/tags/#Deep Learning" title="Deep Learning">Deep Learning</a>
            <span>/</span>
            
            <a class="tag" href="/blog/tags/#Computer Vision" title="Computer Vision">Computer Vision</a>
            <span>/</span>
            
            
        </span>
        <span class="attr">访问：<span id="busuanzi_value_page_pv"></span>
        </span>
        </span>
    </div>
    <div class="post-content ">
        <h3 id="一、背景介绍"><a href="#一、背景介绍" class="headerlink" title="一、背景介绍"></a>一、背景介绍</h3><p>由于手在日常的人类活动中，起着至关重要的作用，估计完整的手的三维姿态越来越重要。在很多场景下，比如运动控制，人机交互，虚拟/增强现实，对手姿态的估计需要在一些混乱、有干扰的环境下进行。由于最近卷积神经网络的发展，目前静态、第三人称视角、在无干扰、无遮挡环境下对手部的追踪和手势估计已经很有效。但很明显，这种场景设定在一些实际场景中并不常见。</p>
<p>在现实世界场景下，经常需要从第一视角来进行手势追踪估计，而且背景往往杂乱无章，手经常在跟物体进行交互时存在遮挡，而需要交互的物体形状不定， 这就为手势估计和重建构成了很多具有挑战性的任务。总结而言，第一人称视角下，进行手势跟踪和估计具有如下一些挑战：</p>
<ul>
<li>存在遮挡</li>
<li>背景杂乱无章(噪声)</li>
<li>第一人称视角带来的视野限制（因为相机往往放置在肩部，如图 1）</li>
<li>手和物体的交互</li>
<li>该场景下，标注数据的缺失</li>
</ul>
<p><img src="http://jackie-image.oss-cn-hangzhou.aliyuncs.com/18-5-3/94908344.jpg" alt></p>
<blockquote>
<p>图 1，第一人称视角的手势跟踪</p>
</blockquote>
<p>本文总结了近几年在第一人称手势估计方面的顶级工作。这些方法基本上都是基于卷积神经网络进行，利用已有的，或者自己采集的，真实的或者合成的数 据进行训练。一般针对与物体交互的场景进行，需要在有遮挡、杂乱无章的环境下进行。</p>
<h3 id="二、手部姿势估计的一般过程"><a href="#二、手部姿势估计的一般过程" class="headerlink" title="二、手部姿势估计的一般过程"></a>二、手部姿势估计的一般过程</h3><p>一般而言，第一人称视角下，手部姿势估计过程可以分成两步，手部定位和姿势估计。近几年，这两者基本上都可以通过卷积神经网络实现。</p>
<p><strong>手部定位</strong>：通过训练好的，多层卷积神经网络，我们可以得到一个关于手部中心位置的置信度分布图，从而可以绘制成热力图的形式，如图 2。</p>
<p><img src="http://jackie-image.oss-cn-hangzhou.aliyuncs.com/18-5-3/61027681.jpg" alt></p>
<blockquote>
<p>图 2，卷积神经网络输出的热力图</p>
</blockquote>
<p><strong>姿势估计</strong>：对手部姿势估计，一般都会采用 21 手关节参数<sup><a href="#fn_1" id="reffn_1">1</a></sup>来估计整个手的姿态。通过手关节的角度，相对位置等信息，可以对整个手姿态进行一个重建，如图 2 b。</p>
<h3 id="三、带标注的训练数据的获取"><a href="#三、带标注的训练数据的获取" class="headerlink" title="三、带标注的训练数据的获取"></a>三、带标注的训练数据的获取</h3><p>一般而言，根据数据获取方式，可以分成三大类，（i）完全合成的数据，（ii）半合成数据以及（iii）真实数据。这些数据大部分是基于 RGB-D 相机获取的，也有搭配手部传感器进行采集。</p>
<h4 id="1-完全合成数据"><a href="#1-完全合成数据" class="headerlink" title="1 完全合成数据"></a>1 完全合成数据</h4><p>因为手动模拟手-物体的交互是一项耗时的任务，故而采用自动的，完全用计算机进行模拟的方法来合成数据。</p>
<p>Choi 等人的工作<sup><a href="#fn_2" id="reffn_2">2</a></sup>中，提出了一种用模型进行拟合的方式，来优化模拟手对于模拟物体的抓握姿势的方法。该工作使用了粒子群优化方法（particle swarm optimization），对虚拟的三维手模型和被该手握住的模型的距离误差进行最小化。之后，作者通过碰撞检测技术来判断这个虚拟手部的抓取是否有效，从而排除无效的抓取。之后作者从模型中导出了相应的手的关节角度参数。最后作者将虚拟手部的深度图插入到杂乱无章的背景中，以模仿现实世界的真是噪音的存在。</p>
<h4 id="2-半合成数据"><a href="#2-半合成数据" class="headerlink" title="2 半合成数据"></a>2 半合成数据</h4><p>为了在真实性和数据的多样性、易获取性上做出权衡，Mueller 等人的工作<sup><a href="#fn_3" id="reffn_3">3</a></sup>则采用了半合成数据的方法，作者称为混合现实（merged reality）。如图 3a 所示，作者用一个无标记跟踪摄像头，用以从第三视角，跟踪一个无遮挡的真实手部，产生跟踪数据（这种方法已经很成熟），从而用以操作虚拟 3D 模型手来抓取虚拟的物体。通过这个方法可以增加数据的真实性。</p>
<p>这种方式的优点在于，既保留了一定的手部运动真实性，又能模拟出大量不同的虚拟手部（比如肤色的不同，手指长短，体毛浓密等等），与各种不同类型物体的交互以及各种不同的场景。</p>
<p><img src="http://jackie-image.oss-cn-hangzhou.aliyuncs.com/18-5-3/74120103.jpg" alt></p>
<blockquote>
<p>图 3，a：混合现实（merged reality）方式获取手部数据，b：使用传感器和 RGB-D 相机 采集第一人称数据</p>
</blockquote>
<h3 id="3-真实数据"><a href="#3-真实数据" class="headerlink" title="3 真实数据"></a>3 真实数据</h3><p>在 Garcia-Hernando<sup><a href="#fn_4" id="reffn_4">4</a></sup>的工作中，采集了超过 100,000 个有 3D 手姿势注释的 RGB-D 帧，使用装在指尖上的六个磁性传感器进行数据采集，如图 3b。其中包括 45 个类别，在 3 个场景中，与 25 个不同的物体进行交互。作者静心设计了不同的手部动作，以保证能覆盖较多的姿势，交互时间和运动状态。</p>
<h3 id="四、手部定位方法（Localization）"><a href="#四、手部定位方法（Localization）" class="headerlink" title="四、手部定位方法（Localization）"></a>四、手部定位方法（Localization）</h3><p>因为卷积神经网络（CNN）方法的成熟，现如今，手部定位基本上都采用卷积神经网络来进行，Choi 等人的工作，也证明了卷积神经网络相对于随机森林方法的优势<sup><a href="#fn_1" id="reffn_1">1</a></sup>。在经过训练好的深度神经网络的估计之后，一般会输出一张关于手部中心位置置信度的热力图。</p>
<p>比如，在 Choi 等人的工作中<sup><a href="#fn_2" id="reffn_2">2</a></sup>，训练了一个带有六个卷积层，以及一个非线性判定层的卷积神经网络，用以判定输入图中的手心位置以及物体位置，如图 4。</p>
<p><img src="http://jackie-image.oss-cn-hangzhou.aliyuncs.com/18-5-3/69002183.jpg" alt></p>
<blockquote>
<p>图 4：由五层卷积层与一层非线性层构成的位置估计卷积网络</p>
</blockquote>
<p>Mueller 等人的工作<sup><a href="#fn_2" id="reffn_2">2</a></sup>，在深度神经网络训练结果上，加入了后处理（post processing）过程，目的是增加数据的稳定性，因为手部中心位置不会随时间的变化而发生较大的变化）。作者保留了同一组数据，多帧图像的历史定位记录，并对每一帧图像的手部位置的定位结果做一个是否可信的判定。如果帧 t 的热力图的最大置信度小于 0.1，且出现的位置和上一次最大值位置距离大于 30，则认为其不可信，然后需要对这个最大值点进行更新：</p>
<script type="math/tex; mode=display">\phi_t=\phi_{t-1}+\delta^k\frac{\phi_{c-1} - \phi_{c-2}}{||\phi_{c-1} - \phi_{c-2}||}</script><p>其中$\phi_t=\phi(H_R^t)$是帧$t$处的更新之后的最大值位置，是$\phi_{c-1}$上一个可置信（confident）的最大值位置，$k$是自上个可置信的最大值起经过的帧数，$\delta$是逐渐对不可信的最大值进行减权（downweight）的衰减因子。</p>
<p>经过更新，最大值位置不会随着时间变化而发生较剧烈的变化。</p>
<h3 id="五、手势估计方法（Hand-Pose-Estimation）"><a href="#五、手势估计方法（Hand-Pose-Estimation）" class="headerlink" title="五、手势估计方法（Hand Pose Estimation）"></a>五、手势估计方法（Hand Pose Estimation）</h3><p>目前有的手势估计方法基本上可以分成两类，基于分类的方法和基于回归的方法。</p>
<h4 id="分类方法"><a href="#分类方法" class="headerlink" title="分类方法"></a>分类方法</h4><p>Rogez 等人在 2014 年发表的文章<sup><a href="#fn_5" id="reffn_5">5</a></sup>中，使用了分层量化的分类器。首先，需要构建一棵姿态类别树，每个节点都代表了一种姿态，层次越高的姿态，普遍性越高。而越接近叶节点，代表其手势的细节越多，更特殊，如图 5 所示。接下去，可以使用宽度优先搜索（BFS）来对输入的手势进行评估，对树的每一层，都可以剪掉那些评级为 0 的节点（也就是没有吻合性的姿态）。</p>
<p><img src="http://jackie-image.oss-cn-hangzhou.aliyuncs.com/18-5-3/60258705.jpg" alt></p>
<blockquote>
<p>图 5，手势层次量化分类器</p>
</blockquote>
<p>Choi 等人的工作<sup><a href="#fn_2" id="reffn_2">2</a></sup>，用一个卷积神经网络，训练了一个分类器，已分类手部姿态，该工作的场景中，用户会抓取一个未知的物体。在训练分类器之前，作者认为，在追踪用户的手部姿态的时候，会因为传感器噪声等，导致输出的 RGB-D 图像存在一定缺失。故而在训练该分类器的同时，训练了输入数据再生成网络，通过自动编码器的方法实现输入数据复原。自动编码器由编码器（将高维数据映射到较低的维特征空间来降低输入的维数）和解码器（通过将学到的表示，映射回高维空间来恢复原始输入）构成，两者都是卷积神经网络，作者为两者都加入了四层隐含层，如图 6a。之后，作者认为手的姿态和物体形状高度相关，所以在训练手部姿态分类器的同时，也加入了另一个神经网络作为物体形状的分类器，并共享这两者的决策层，来协作学习手和物体的成对的卷积特征，以增加分类准确性，如图 6a。</p>
<p><img src="http://jackie-image.oss-cn-hangzhou.aliyuncs.com/18-5-3/74747573.jpg" alt></p>
<blockquote>
<p>图 6，a：RGB-D 图 数据再生成网络，b：手势分类器神经网络的架构</p>
</blockquote>
<h4 id="回归方法"><a href="#回归方法" class="headerlink" title="回归方法"></a>回归方法</h4><p>在 Mueller<sup><a href="#fn_3" id="reffn_3">3</a></sup>等人的工作中，则是使用了卷积神经网络，对手部姿态向量进行了回归。作者采用了一个 26 自由度的手骨骼模型，其包括 6 个用于全局平移和旋转的角度，以及 20 个关节角度，存储在向量$\Theta$中，然后用一个卷积神经网络对这个向量进行了回归。Mueller 等人在这个基础上，还加入了一些约束（比如关节旋转角度的范围，手关节运动速度等），以保证回归结果的有效性。</p>
<script type="math/tex; mode=display">\mathcal{E}(\Theta) = E_{data}(\Theta,p_G,H)+E_{reg}(\Theta)</script><p>其中，$E_{data}$约束了相对位置，$E_{reg}$约束了关节旋转角度和运动速度。</p>
<blockquote id="fn_1">
<sup>1</sup>. Choi, Chiho, et al. “A collaborative filtering approach to real-time hand pose estimation.” _Proceedings of the IEEE International Conference on Computer Vision_. 2015.<a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_2">
<sup>2</sup>. Choi, Chiho, et al. “Robust hand pose estimation during the interaction with an unknown object.” _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_. 2017.<a href="#reffn_2" title="Jump back to footnote [2] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_3">
<sup>3</sup>. Mueller, Franziska, et al. “Real-time hand tracking under occlusion from an egocentric rgb-d sensor.” _Proceedings of International Conference on Computer Vision (ICCV)_. Vol. 10. 2017.<a href="#reffn_3" title="Jump back to footnote [3] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_4">
<sup>4</sup>. Garcia-Hernando, Guillermo, et al. “First-Person Hand Action Benchmark with RGB-D Videos and 3D Hand Pose Annotations.” _arXiv preprint arXiv:1704.02463_(2017).<a href="#reffn_4" title="Jump back to footnote [4] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_5">
<sup>5</sup>. Rogez, Grégory, et al. “3d hand pose detection in egocentric rgb-d images.” _Workshop at the European conference on computer vision_. Springer, Cham, 2014.<a href="#reffn_5" title="Jump back to footnote [5] in the text."> &#8617;</a>
</blockquote>

        
        <br />
        <div id="comment-container">
        </div>
        <div id="disqus_thread"></div>

        <div id="lv-container">
        </div>

    </div>
</div>
    </div>
    
    
    <div class="index-right">
        <div id="toc" class="toc-article">
    <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#一、背景介绍"><span class="toc-text">一、背景介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#二、手部姿势估计的一般过程"><span class="toc-text">二、手部姿势估计的一般过程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#三、带标注的训练数据的获取"><span class="toc-text">三、带标注的训练数据的获取</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-完全合成数据"><span class="toc-text">1 完全合成数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-半合成数据"><span class="toc-text">2 半合成数据</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-真实数据"><span class="toc-text">3 真实数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#四、手部定位方法（Localization）"><span class="toc-text">四、手部定位方法（Localization）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#五、手势估计方法（Hand-Pose-Estimation）"><span class="toc-text">五、手势估计方法（Hand Pose Estimation）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#分类方法"><span class="toc-text">分类方法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#回归方法"><span class="toc-text">回归方法</span></a></li></ol></li></ol>
</div>
    </div>
    
</div>

<footer class="footer">
  <ul class="list-inline text-center">
         
  </ul>
  
  <p>
    <a href="http://panjiacheng.site">@Jiacheng Pan</a> Created By
    <a href="https://hexo.io/">Hexo</a> Theme
    <a href="https://github.com/aircloud/hexo-theme-aircloud">AirCloud</a>
    <a href="https://github.com/aircloud">@Xiaotao</a>
  </p>
</footer><!-- hexo-inject:begin --><!-- hexo-inject:end -->


</body>

<script>
    // We expose some of the variables needed by the front end
    window.hexo_search_path = "search.json"
    window.hexo_root = "/blog/"
    window.isPost = true
</script>
<script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script>
<script src="/blog/js/index.js"></script>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




    <script type="text/javascript">
       (function() {
           if (typeof LivereTower === 'function') { return; }

           var j, d = document.getElementById('lv-container');

           d.setAttribute('data-id','city');
           d.setAttribute('data-uid' , 'MTAyMC8yOTk2MC82NTI1');

           j = document.createElement('script');
           j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
           j.async = true;

           d.appendChild(j);
       })();
    </script>
    <noscript>为正常使用来必力评论功能请激活JavaScript</noscript>
    </div>

</html>
