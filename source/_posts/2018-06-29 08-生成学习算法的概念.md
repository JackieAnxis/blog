---
title: CS229：08-生成学习算法的概念
date: 2018-06-29 12:40:00
tags: ["Machine Learning", "Course Notes"]
author: AndrewNgCS229
mathjax: true
---

生成学习算法，英文为*Generative Learning Algorithm*。

我们之前看到的都是判别学习算法（_Discriminative Learning Algorithm_）。判别学习算法可以分成两种：

1. 学得$p(y|x)$，比如之前的线性模型
2. 学得一个假设$h_\theta (x) = \lbrace 0, 1 \rbrace$，比如二分类问题

上面都是根据特征$x$，来对输出$y$进行建模，也许$y$是一个连续的值，也可能是离散的，比如类别。

那么，生成学习算法（_Generative Learning Algorithm_）刚好跟判别学习算法（_Discriminative Learning Algorithm_）相反，其用输出$y$对$x$进行建模，也就是：**学得$p(x|y)$**。

**详细解释**：

对于一个二分类或多分类问题，生成学习算法，在给定样本所述的类别的条件下，会对其样本特征建立一个概率模型。举例而言，在给定癌症是良性或者是恶性的条件下，生成模型会对该癌症的特征的概率分布进行建模。

根据朴素贝叶斯，

$$
p(y=1|x)=\frac{p(x|y=1) p(y=1)}{p(x)}
$$

因为给定了$x$，所以$p(x)$可以视作 1，也即

$$
p(y=1|x)=p(x|y=1) p(y=1)
$$

## 高斯判别算法

_Gaussian Discriminant Algorithm_

对特征满足高斯分布的二分类问题进行建模。

假设$x \in \Bbb R^n$，且$p(x|y)$满足高斯分布，因为$x$往往是一个特征向量，故而这里的高斯分布是一个多元高斯分布（_multivariate Gaussian）_。

> 多元高斯分布，$z \sim N(\mu, \Sigma)$。
>
> $$
> p(z)=\frac{1}{\sqrt{(2\pi)^n|\Sigma|}}exp(-\frac{1}{2}(z-\mu)^T\Sigma^{-1}(z-\mu))
> $$
>
> $z$是一个$n$维向量，$\mu$则是均值向量，$\Sigma$是协方差矩阵：$E[(z-\mu)(z-\mu)^T]$。

对于一个二分类判别问题，正则响应函数为$\phi$，那么

$$
p(y)=\phi^y(1-\phi)^{1-y}
$$

且

$$
p(x|y=0) \sim N(\mu_0, \Sigma) \\\
p(x|y=1) \sim N(\mu_1, \Sigma)
$$

(这里假设了，对于$y=0$和$y=1$，是两个不同均值，但协方差矩阵相同的多元高斯分布，所以这也是该模型的限制之一)

我们写出对数似然函数：

$$
\begin{align}
l(\phi, \mu_0, \mu_1, \Sigma) &= log\prod_{i=1}^mp(x^{(i)}, (y^{(i)})) \\\
&= log\prod_{i=1}^mp(x^{(i)}| (y^{(i)}))p(y^{(i)})
\end{align}
$$

对比一下之前二分类问题中的对数似然估计函数：

$$
l(\theta) = \sum_{i=1}^mlog(P(y^{(i)}|x^{(i)};\theta))
$$

这里，我们使用了联合概率（_joint likelihood_）: $p(x^{(i)}, (y^{(i)}))$，而在之前的判别模型里，我们用的是条件概率（_conditional likelihood_）: $P(y^{(i)}|x^{(i)};\theta)$。因为，在判别模型中，特征$x$是给定的，所以用条件概率来进行极大似然估计；而在生成模型中，特征$x$不给定，所以需要用联合概率来进行极大似然估计。

然后，我们进行极大似然估计，得到：

$$
\phi=\frac{1}{m}\sum_{i=1}^my^{(i)}=\frac{1}{m}\sum_{i=1}^m 1\lbrace y^{(i)} = 1 \rbrace
$$

也就是分类标签为 1 的训练样本的比例。

再看$\mu_0, \mu_1$：

$$
\mu_0=\underbrace{\sum_{i=1}^m 1\lbrace y^{(i)} = 0 \rbrace \cdot x^{(i)}}_{(1)} /\underbrace{ \sum_{i=1}^m 1\lbrace y^{(i)} = 0 \rbrace}_{(2)} \\\
\mu_1=\sum_{i=1}^m 1\lbrace y^{(i)} = 1 \rbrace \cdot x^{(i)} / \sum_{i=1}^m 1\lbrace y^{(i)} = 1 \rbrace
$$

式(1)表达了分类为 0 的样本中，特征$x$的和；式(2)表达了分类为 0 的样本个数；故而$\mu_0$表达了样本中，分类为 0 的样本的特征的均值。同理得到$\mu_1$。

我们再次回顾上面的高斯判别算法，事实上，高斯判别算法，假设了特征$x$满足了多元高斯分布，利用极大似然估计，对不同类别的特征$x$的分布进行了建模。

下节课我们将会探讨其他的关于生成学习算法的案例。
