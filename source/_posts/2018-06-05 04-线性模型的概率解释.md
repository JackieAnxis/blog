---
title: CS229：04-线性模型的概率解释
date: 2018-06-05 11:02:00
tags: ["Machine Learning", "Course Notes"]
author: AndrewNgCS229
mathjax: true
---

关于：为何在进行线性回归时，选择用最小二乘拟合（距离的平方和）来进行，而不是选用其他的模型（比如三次方或四次方）？

我们更新一下假设函数，使之变为：

$$
y^{(i)} = \theta^Tx^{(i)} + \varepsilon^{(i)}
$$

其中，$\varepsilon^{(i)}$是误差项，表示未捕获的特征（unmodeled effects），比如房子存在壁炉也影响价格，或者其他的一些随机噪音（random noise）。

一般，会假设误差项$\varepsilon^{(i)} \sim N(0, \sigma^2)$（满足正态分布），也就是：

$$
P(\varepsilon^{(i)})=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(\varepsilon^{(i)})^2}{2\sigma^2})
$$

关于为什么假设正态分布的解释：

1. 便于数学运算；
2. 很多独立分布的变量之间相互叠加后会趋向于正态分布（中心极限定理），在大多数情况下能成立

所以，$y^{(i)}$的后验分布：

$$
P(y^{(i)}|x^{(i)};\theta)=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}) \sim N(\theta^Tx^{(i)}, \sigma^2)
$$

之后，进行极大似然估计（maximum likelihood estimation）：$max L(\theta)$，即选择合适的$\theta$，使得$y^{(i)}$对于$x^{(i)}$出现的概率最高（有一些存在即合理的感觉），其中$L(\theta)$的定义如下：

$$
L(\theta)=P(y|x;\theta)=\prod_{i=1}^mP(y^{(i)}|x^{(i)};\theta)=\prod_{i=1}^m\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})
$$

那么，为了计算方便，我们定义：

$$
l(\theta) = log(L(\theta))=\sum_{i=1}^mlog(P(y^{(i)}|x^{(i)};\theta))=m\cdot log(\frac{1}{\sqrt{2\pi}\sigma})-\sum_{i=1}^m\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}
$$

于是，极大似然估计变为最小化：

$$
\sum_{i=1}^m\frac{(y{(i)}-\theta^Tx{(i)})2}{2\sigma2}
$$

也即之前线性回归所需进行最小二乘拟合的$J(\theta)$。
