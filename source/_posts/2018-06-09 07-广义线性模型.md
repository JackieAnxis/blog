---
title: CS229：07-广义线性模型
date: 2018-06-09 16:29:00
tags: ["Machine Learning", "Course Notes"]
author: AndrewNgCS229
mathjax: true
---

广义线性模型，英文名为**Generalized Linear Model**，简称 GLM。

之前，涉及到两种的两种模型：

1. 线性拟合模型，假设了$P(y|x;\theta)$是高斯分布
2. 二分类问题，假设了$P(y|x;\theta)$满足伯努利分布

但以上两者知识一种更广泛的，被称为『指数分布族』（The Exponential Family）的特例。

## 指数分布族

$$
P(y;\eta)=b(y)exp(\eta^TT(y)-a(\eta))
$$

可以被表示为以上形式的分布，都是指数分布族的某个特定分布，给定$a, b, T$，就可以定义一个概率分布的集合，以$\eta$为参数，就可以得到不同的概率分布。

在广义线性模型中，会假设$\eta=\theta^Tx$，也就是$\eta$和特征$x$线性相关。

## 伯努利分布

首先，我们给出$y=1$的概率：

$$
P(y=1;\phi)=\phi
$$

于是：

$$
\begin{align}
P(y;\phi)
	&= \phi^y(1-\phi)^T\\\
	&= exp(log(\phi^T(1-\phi^T)))\\\
	&= exp(ylog(\phi)+(1-y)log(1-\phi))\\\
	&= exp(log\frac{\phi}{1-\phi} \cdot y + log(1-\phi))
\end{align}
$$

比较我们上面的概率形式和指数分布族的标准形式，可以得到：

$$
\begin{cases}
\eta &= log\frac{\phi}{1-\phi}, \text{于是} \phi=\frac{1}{1+e^{-\eta}}\\\
a(\eta) &= -log(1-\phi)=log(1+e^\eta)\\\
T(y) &= y\\\
b(y) &= 1
\end{cases}
$$

这里的$\phi$一般会被称为正则响应函数（_canonic response function_）：

$$
g(\eta) = E[y|\eta]=\frac{1}{1+e^{-\eta}}
$$

相对的，正则关联函数（_canonic link function_）则是$g^{-1}$。

## 高斯分布

$$
N(\mu,\sigma^2)=\frac{1}{\sqrt{2\pi}}exp(-\frac{1}{2}(y-\mu)^2)
$$

这里，出于简洁考虑，假设$\sigma=1$，经过一系列化简后，可以表示成：

$$
\frac{1}{\sqrt{2\pi}} \cdot exp(-\frac{1}{2}y^2) \cdot exp(\mu y-\frac{1}{2}\mu^2)
$$

那么，

$$
\begin{cases}
\eta &= \mu\\\
a(\eta) &= \frac{1}{2}\mu^2=\frac{1}{2}\eta^2\\\
T(y) &= y\\\
b(y) &= \frac{1}{\sqrt{2\pi}} \cdot exp(-\frac{1}{2}y^2)
\end{cases}
$$

## 多项式分布

#### 建模

在二项分布中，$y\in \lbrace 1, 2 \rbrace$

而多项式分布，$y \in \lbrace 1,\cdots, k \rbrace$

一般会被用来进行邮件分类或者进行病情分类等等

我们假设

$$
P(y=i)=\phi_i
$$

也即，邮件属于$i$类的概率是$\phi_i$，是关于特征$x$的一个函数。

那么，可以用$k$个参数来建模多项式分布

$$
P(y)=\prod_{i=1}^k\phi_i^{1\lbrace y=i \rbrace}
$$

其中，$1 \lbrace \cdots \rbrace$的含义为，检验$\cdots$是否为真命题，若为真命题，则取 1，否则取 0。

因为所有概率和为 1，所以最后一个参数

$$
\begin{align}
\phi_k &= 1-\sum_{j=1}^{k-1}\phi_j \\\
1 \lbrace y=k \rbrace &=1-\sum_{j=1}^{k-1}1 \lbrace y=j \rbrace
\end{align}
$$

经过化简，也可以表示成：

$$
P(y)=exp[\sum_{i=1}^{k-1}(log(\frac{\phi_i}{\phi_k}) \cdot 1\lbrace y=i \rbrace )] + log(\phi_k)
$$

故而

$$
\eta = \begin{bmatrix}
log(\frac{\phi_1}{\phi_k}) \\\
\vdots \\\
log(\frac{\phi_{k-1}}{\phi_k})
\end{bmatrix} \in \Bbb R^{k-1}
$$

$$
a(\eta) = -log(\phi_k)
$$

$$
T(y)= \begin{bmatrix}
1 \lbrace y=1 \rbrace \\\
\vdots \\\
1 \lbrace y=k-1 \rbrace
\end{bmatrix} \in (0, 1)^{k-1}
$$

$$
b(y) = 1
$$

根据$\eta$可得：

$$
\phi_i = e^{\eta_i} \cdot \phi_k
$$

又因为：

$$
\sum_{i=1}^{k}\phi_i=\sum_{i=1}^k\phi_ke^{\eta_i}=1
$$

故而：

$$
\phi_k = \frac{1}{\sum_{i=1}^ke^{\eta_i}}=\frac{1}{e^{\eta_k}+\sum_{i=1}^{k-1}e^{\eta_i}} = \frac{1}{1+\sum_{i=1}^{k-1}e^{\eta_i}}
$$

所以：

$$
\begin{align}
\phi_i &= e^{\eta_i} \cdot \phi_k \\\
&= \frac{e^{\eta_i}}{1 + \sum_{j=1}^{k-1}e^{\eta_j}} \\\
&= \frac{e^{\theta_i^Tx_i}}{1 + \sum_{j=1}^{k-1}e^{\theta_j^Tx_j}}
\end{align}
$$

上述函数，被称为『softmax』函数，这个函数的作用经常用于进行归一化。

经过上述步骤，假设函数可以被写成如下形式：

$$
h_\theta(x)=
\left[
\begin{array}{c}
1\lbrace y=1 \rbrace  \\\
\vdots \\\
1\lbrace y=k-1 \rbrace
\end{array} | x;\theta
\right]=
\begin{bmatrix}
\phi_1\\\
\vdots\\\
\phi_{k-1}
\end{bmatrix}
$$

#### 回归

在经过上述推导，当我们有一堆训练集（$(x^{(1)}, y^{(1)}), \cdots, (x^{(m)}, y^{(m)})$）用于训练的时候，则可以进行极大似然估计：

$$
L(\theta) = \prod_{i=1}^mP(y^{(i)} | x^{(i)};\theta) = \prod_{i=1}^m\prod_{j=1}^k\phi_j^{1\lbrace y^{(i)}=j \rbrace }
$$
